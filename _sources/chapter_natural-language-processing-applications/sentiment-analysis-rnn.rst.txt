
.. _sec_sentiment_rnn:

情感分析：使用循环神经网络
==========================


与词相似度和类比任务一样，我们也可以将预先训练的词向量应用于情感分析。由于
:numref:`sec_sentiment`\ 中的IMDb评论数据集不是很大，使用在大规模语料库上预训练的文本表示可以减少模型的过拟合。作为
:numref:`fig_nlp-map-sa-rnn`\ 中所示的具体示例，我们将使用预训练的GloVe模型来表示每个词元，并将这些词元表示送入多层双向循环神经网络以获得文本序列表示，该文本序列表示将被转换为情感分析输出
:cite:`Maas.Daly.Pham.ea.2011`\ 。对于相同的下游应用，我们稍后将考虑不同的架构选择。

.. _fig_nlp-map-sa-rnn:

.. figure:: ../img/nlp-map-sa-rnn.svg

   将GloVe送入基于循环神经网络的架构，用于情感分析


.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    import mlx.core as mx
    import mlx.nn as nn
    import mlx.optimizers as optim
    from d2l import mlx as d2l
    
    batch_size = 64
    train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)

使用循环神经网络表示单个文本
----------------------------

在文本分类任务（如情感分析）中，可变长度的文本序列将被转换为固定长度的类别。在下面的\ ``BiRNN``\ 类中，虽然文本序列的每个词元经由嵌入层（\ ``self.embedding``\ ）获得其单独的预训练GloVe表示，但是整个序列由双向循环神经网络（\ ``self.encoder``\ ）编码。更具体地说，双向长短期记忆网络在初始和最终时间步的隐状态（在最后一层）被连结起来作为文本序列的表示。然后，通过一个具有两个输出（“积极”和“消极”）的全连接层（\ ``self.decoder``\ ），将此单一文本表示转换为输出类别。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    class BiRNN(nn.Module):
        def __init__(self, vocab_size, embed_size, num_hiddens,
                     num_layers, **kwargs):
            super(BiRNN, self).__init__(**kwargs)
            self.embedding = nn.Embedding(vocab_size, embed_size)
            # 将bidirectional设置为True以获取双向循环神经网络
            self.encoder = d2l.MultiLayerLSTM(embed_size, num_hiddens, num_layers=num_layers,
                                    bidirectional=True)
            self.decoder = nn.Linear(4 * num_hiddens, 2)
    
        def __call__(self, inputs):
            # inputs的形状是（批量大小，时间步数）
            # 输出形状为（批量大小，时间步数，词向量维度）
            embeddings = self.embedding(inputs)
            # 返回上一个隐藏层在不同时间步的隐状态，
            # outputs的形状是（批量大小，时间步数，2*隐藏单元数）
            outputs, _ = self.encoder(embeddings)
            # 连结初始和最终时间步的隐状态，作为全连接层的输入，
            # 其形状为（批量大小，4*隐藏单元数）
            encoding = mx.concatenate((outputs[:,0,:], outputs[:,-1,:]), axis=1)
            outs = self.decoder(encoding)
            return outs

让我们构造一个具有两个隐藏层的双向循环神经网络来表示单个文本以进行情感分析。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    embed_size, num_hiddens, num_layers = 100, 100, 2
    net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    # def init_weights(m):
    #     if type(m) == nn.Linear:
    #         nn.init.xavier_uniform_(m.weight)
    #     if type(m) == nn.LSTM:
    #         for param in m._flat_weights_names:
    #             if "weight" in param:
    #                 nn.init.xavier_uniform_(m._parameters[param])
    # net.apply(init_weights);
    
    # from mlx.utils import tree_flatten,tree_unflatten
    
    # def init_weights(net):
    #     weight_fn = nn.init.glorot_uniform()
    #     bias_fn = nn.init.constant(0.0)
    #     for name, param in net.parameters().items():
    #         if "weight" in name:
    #             if len(param.shape) >= 2:
    #                 # # 使用 Xavier 初始化
    #                 param[...] = weight_fn(param)
    #         elif "bias" in name:
    #             param[...] = bias_fn(param)
    
    # def init_weights(m):
    #     weight_fn = nn.init.glorot_uniform()
    #     bias_fn = nn.init.constant(0.0)
    
    #     if type(m) == nn.Linear:
    #         m.weigth = weight_fn(m.weight)
    #         m.bias = bias_fn(m.bias)
    #     if type(m) == nn.LSTM:
    #         for name, param in tree_flatten(m.parameters()):
    #             if "weight" in name:
    #                 nn.init.xavier_uniform_(param)
    #             elif "bias" in name:
    #                 nn.init.constant_(param, 0.0)
    
    from mlx.utils import tree_map_with_path
    
    def init_fn(path, a):
        if "embedding" in path:
            return nn.init.normal(mean=0.0, std=0.02)(a)
        elif "bias" in path:
            return nn.init.constant(0.)(a)
        elif "ln" in path:
            return nn.init.constant(1.)(a)
        elif "ffn.layers.0" in path:
            return nn.init.he_uniform(a.dtype)(a, 'fan_in', 1.),
        return nn.init.glorot_uniform(a.dtype)(a, 1.)
    
    net.update(tree_map_with_path(init_fn, net.parameters()))




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    BiRNN(
      (embedding): Embedding(49346, 100)
      (encoder): MultiLayerLSTM(
        (fwd_lstm_0): LSTM(input_dims=100, hidden_size=100, bias=True)
        (bwd_lstm_0): LSTM(input_dims=100, hidden_size=100, bias=True)
        (fwd_lstm_1): LSTM(input_dims=200, hidden_size=100, bias=True)
        (bwd_lstm_1): LSTM(input_dims=200, hidden_size=100, bias=True)
      )
      (decoder): Linear(input_dims=400, output_dims=2, bias=True)
    )



加载预训练的词向量
------------------

下面，我们为词表中的单词加载预训练的100维（需要与\ ``embed_size``\ 一致）的GloVe嵌入。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    glove_embedding = d2l.TokenEmbedding('glove.6b.100d')

打印词表中所有词元向量的形状。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    embeds = glove_embedding[vocab.idx_to_token]
    embeds.shape




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    (49346, 100)



我们使用这些预训练的词向量来表示评论中的词元，并且在训练期间不要更新这些向量。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    net.embedding.weight= mx.array(embeds)
    net.embedding.freeze()




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Embedding(49346, 100)



训练和评估模型
--------------

现在我们可以训练双向循环神经网络进行情感分析。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    # 获取批量数
    def get_num_batch(iter):
        num_batches = 0
        for sample in iter:
            num_batches += 1
        iter.reset()
        return num_batches

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    lr, num_epochs = 0.01, 5
    trainer = optim.Adam(learning_rate=lr)
    loss = nn.losses.cross_entropy
    num_batches = get_num_batch(train_iter)
    d2l.train_ch13(net, train_iter,num_batches, test_iter, loss, trainer, num_epochs)


::


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    Cell In[9], line 5
          3 loss = nn.losses.cross_entropy
          4 num_batches = get_num_batch(train_iter)
    ----> 5 d2l.train_ch13(net, train_iter,num_batches, test_iter, loss, trainer, num_epochs)


    File /opt/miniconda3/envs/d2l-zh/lib/python3.10/site-packages/d2l/mlx.py:2917, in train_ch13(net, train_iter, num_train_batches, test_iter, loss, trainer, num_epochs)
       2915 features, labels = mx.array(samples["X"]), mx.array(samples["y"])
       2916 timer.start()
    -> 2917 l, acc = train_batch_ch13(
       2918     net, features, labels, train_step, trainer)
       2920 metric.add(l, acc, labels.shape[0], labels.size)
       2921 timer.stop()


    File /opt/miniconda3/envs/d2l-zh/lib/python3.10/site-packages/d2l/mlx.py:2891, in train_batch_ch13(net, X, y, train_step, trainer)
       2888 (loss, acc), grads = loss_and_grad_fn(net, X, y)
       2889 trainer.update(net, grads)
    -> 2891 train_loss_sum = loss.item()
       2892 train_acc_sum = acc.item()
       2893 return train_loss_sum, train_acc_sum


    KeyboardInterrupt: 



.. figure:: output_sentiment-analysis-rnn_e3f486_15_1.svg


我们定义以下函数来使用训练好的模型\ ``net``\ 预测文本序列的情感。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    #@save
    def predict_sentiment(net, vocab, sequence):
        """预测文本序列的情感"""
        sequence = mx.array(vocab[sequence.split()])
        label = mx.argmax(net(sequence.reshape(1, -1)), axis=1)
        return 'positive' if label == 1 else 'negative'

最后，让我们使用训练好的模型对两个简单的句子进行情感预测。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    predict_sentiment(net, vocab, 'this movie is so great')




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    'negative'



.. raw:: latex

   \diilbookstyleinputcell

.. code:: ipython3

    predict_sentiment(net, vocab, 'this movie is so bad')




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    'negative'



小结
----

- 预训练的词向量可以表示文本序列中的各个词元。
- 双向循环神经网络可以表示文本序列。例如通过连结初始和最终时间步的隐状态，可以使用全连接的层将该单个文本表示转换为类别。

练习
----

1. 增加迭代轮数可以提高训练和测试的准确性吗？调优其他超参数怎么样？
2. 使用较大的预训练词向量，例如300维的GloVe嵌入。它是否提高了分类精度？
3. 是否可以通过spaCy词元化来提高分类精度？需要安装Spacy（\ ``pip install spacy``\ ）和英语语言包（\ ``python -m spacy download en``\ ）。在代码中，首先导入Spacy（\ ``import spacy``\ ）。然后，加载Spacy英语软件包（\ ``spacy_en = spacy.load('en')``\ ）。最后，定义函数\ ``def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]``\ 并替换原来的\ ``tokenizer``\ 函数。请注意GloVe和spaCy中短语标记的不同形式。例如，短语标记“new
   york”在GloVe中的形式是“new-york”，而在spaCy词元化之后的形式是“new
   york”。

`Discussions <https://discuss.d2l.ai/t/5724>`__
