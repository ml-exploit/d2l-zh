
.. _sec_word2vec_pretraining:

预训练word2vec
==============


我们继续实现
:numref:`sec_word2vec`\ 中定义的跳元语法模型。然后，我们将在PTB数据集上使用负采样预训练word2vec。首先，让我们通过调用\ ``d2l.load_data_ptb``\ 函数来获得该数据集的数据迭代器和词表，该函数在
:numref:`sec_word2vec_data`\ 中进行了描述。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    import math
    import mlx.core as mx
    import mlx.optimizers as optim
    from mlx import nn
    from d2l import mlx as d2l
    
    batch_size, max_window_size, num_noise_words = 512, 5, 5
    data_iter, num_batches, vocab = d2l.load_data_ptb(batch_size, max_window_size,
                                         num_noise_words)

跳元模型
--------

我们通过嵌入层和批量矩阵乘法实现了跳元模型。首先，让我们回顾一下嵌入层是如何工作的。

嵌入层
~~~~~~

如
:numref:`sec_seq2seq`\ 中所述，嵌入层将词元的索引映射到其特征向量。该层的权重是一个矩阵，其行数等于字典大小（\ ``input_dim``\ ），列数等于每个标记的向量维数（\ ``output_dim``\ ）。在词嵌入模型训练之后，这个权重就是我们所需要的。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed = nn.Embedding(num_embeddings=20, dims=4)
    print(f'Parameter embedding_weight ({embed.weight.shape}, '
          f'dtype={embed.weight.dtype})')


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    Parameter embedding_weight ((20, 4), dtype=mlx.core.float32)


嵌入层的输入是词元（词）的索引。对于任何词元索引\ :math:`i`\ ，其向量表示可以从嵌入层中的权重矩阵的第\ :math:`i`\ 行获得。由于向量维度（\ ``output_dim``\ ）被设置为4，因此当小批量词元索引的形状为（2，3）时，嵌入层返回具有形状（2，3，4）的向量。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    x = mx.array([[1, 2, 3], [4, 5, 6]])
    embed(x)




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    array([[[0.303984, 0.531017, -0.495427, -0.175654],
            [-0.711812, -0.30421, -0.0697758, -0.0844888],
            [-0.00825905, 0.275464, 0.458484, 0.250131]],
           [[-0.0981255, -0.0445608, 1.58152, 0.462035],
            [-0.117063, 0.512992, 0.632484, -0.873858],
            [-0.0468011, 0.0142339, 0.198256, 0.214218]]], dtype=float32)



定义前向传播
~~~~~~~~~~~~

在前向传播中，跳元语法模型的输入包括形状为（批量大小，1）的中心词索引\ ``center``\ 和形状为（批量大小，\ ``max_len``\ ）的上下文与噪声词索引\ ``contexts_and_negatives``\ ，其中\ ``max_len``\ 在
:numref:`subsec_word2vec-minibatch-loading`\ 中定义。这两个变量首先通过嵌入层从词元索引转换成向量，然后它们的批量矩阵相乘（在
:numref:`subsec_batch_dot`\ 中描述）返回形状为（批量大小，1，\ ``max_len``\ ）的输出。输出中的每个元素是中心词向量和上下文或噪声词向量的点积。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def skip_gram(center, contexts_and_negatives, embed_v, embed_u):
        v = embed_v(center)  # 第一个Embedding层处理中心词
        u = embed_u(contexts_and_negatives) # 第二个Embedding层处理上下文词和负样本词
        # 使用einsum进行批量矩阵乘法
        # 'bij,bkj->bik' 表示: batch维度保持，i和k维度相乘，j维度求和
        pred = mx.einsum('bij,bkj->bik', v, u)
        return pred

让我们为一些样例输入打印此\ ``skip_gram``\ 函数的输出形状。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    skip_gram(mx.ones((2, 1), dtype=mx.int32),
              mx.ones((2, 4), dtype=mx.int32), embed, embed).shape




.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    (2, 1, 4)



训练
----

在训练带负采样的跳元模型之前，我们先定义它的损失函数。

二元交叉熵损失
~~~~~~~~~~~~~~

根据
:numref:`subsec_negative-sampling`\ 中负采样损失函数的定义，我们将使用二元交叉熵损失。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    loss = nn.losses.binary_cross_entropy

回想一下我们在
:numref:`subsec_word2vec-minibatch-loading`\ 中对掩码变量和标签变量的描述。下面计算给定变量的二进制交叉熵损失。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    pred = mx.array([[1.1, -2.2, 3.3, -4.4]] * 2)
    label = mx.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])
    mask = mx.array([[1, 1, 1, 1], [1, 1, 0, 0]])
    print(loss(inputs=pred, targets=label, weights=mask, with_logits=True, reduction='none').sum(axis=1) / mask.sum(axis=1))


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    array([0.93521, 1.84621], dtype=float32)


下面显示了如何使用二元交叉熵损失中的Sigmoid激活函数（以较低效率的方式）计算上述结果。我们可以将这两个输出视为两个规范化的损失，在非掩码预测上进行平均。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def sigmd(x):
        return -math.log(1 / (1 + math.exp(-x)))
    
    print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')
    print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    0.9352
    1.8462


初始化模型参数
~~~~~~~~~~~~~~

我们定义了两个嵌入层，将词表中的所有单词分别作为中心词和上下文词使用。字向量维度\ ``embed_size``\ 被设置为100。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    embed_size = 100
    net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),
                                     dims=embed_size),
                        nn.Embedding(num_embeddings=len(vocab),
                                     dims=embed_size))

定义训练阶段代码
~~~~~~~~~~~~~~~~

训练阶段代码实现定义如下。由于填充的存在，损失函数的计算与以前的训练函数略有不同。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def train(net, data_iter, num_batches, lr, num_epochs):
        optimizer = optim.Adam(learning_rate=lr)
        animator = d2l.Animator(xlabel='epoch', ylabel='loss',
                                xlim=[1, num_epochs])
        def loss_fn(net, center, context_negative, label, mask):
            embeddings = net.children()["layers"]
            pred = skip_gram(center, context_negative, embeddings[0], embeddings[1])
            l = loss(inputs=pred.reshape(label.shape),
                     targets=label, weights=mask,
                     with_logits=True,
                     reduction='none').sum(axis=1) / mask.sum(axis=1)
            return mx.sum(l) / l.shape[0]
        # 规范化的损失之和，规范化的损失数
        metric = d2l.Accumulator(2)
        for epoch in range(num_epochs):
            timer, num_batches = d2l.Timer(), num_batches
            for i, batch in enumerate(data_iter):
                center, context_negative, mask, label = mx.array(batch["centers"]), mx.array(batch["contexts_negatives"]), mx.array(batch["masks"]), mx.array(batch["labels"])
                loss_and_grad_fn = nn.value_and_grad(net, loss_fn)
                l, grads = loss_and_grad_fn(net, center, context_negative, label, mask)
                optimizer.update(net, grads)
                metric.add(l, l.size)
                if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:
                    animator.add(epoch + (i + 1) / num_batches,
                                 (metric[0] / metric[1],))
            data_iter.reset()
        print(f'loss {metric[0] / metric[1]:.3f}, '
              f'{metric[1] / timer.stop():.1f} tokens/sec')

现在，我们可以使用负采样来训练跳元模型。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    lr, num_epochs = 0.002, 5
    train(net, data_iter, num_batches, lr, num_epochs)


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    loss 0.382, 1270.9 tokens/sec



.. figure:: output_word2vec-pretraining_8b7dee_21_1.svg


.. _subsec_apply-word-embed:

应用词嵌入
----------


在训练word2vec模型之后，我们可以使用训练好模型中词向量的余弦相似度来从词表中找到与输入单词语义最相似的单词。

.. raw:: latex

   \diilbookstyleinputcell

.. code:: python

    def get_similar_tokens(query_token, k, embed):
        W = embed.weight
        x = W[vocab[query_token]]
        # 计算余弦相似性。增加1e-9以获得数值稳定性
        cos = (W @ x) / mx.sqrt(mx.sum(W * W, axis=1) *
                                          mx.sum(x * x) + 1e-9)
        topk = mx.argsort(-cos)[:k+1]
        for i in topk[1:]:
            print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i.item())}')
    
    get_similar_tokens('chip', 3, net.children()["layers"][0])


.. raw:: latex

   \diilbookstyleoutputcell

.. parsed-literal::
    :class: output

    cosine sim=0.555: optical
    cosine sim=0.549: microprocessor
    cosine sim=0.511: desktop


小结
----

-  我们可以使用嵌入层和二元交叉熵损失来训练带负采样的跳元模型。
-  词嵌入的应用包括基于词向量的余弦相似度为给定词找到语义相似的词。

练习
----

1. 使用训练好的模型，找出其他输入词在语义上相似的词。您能通过调优超参数来改进结果吗？
2. 当训练语料库很大时，在更新模型参数时，我们经常对当前小批量的\ *中心词*\ 进行上下文词和噪声词的采样。换言之，同一中心词在不同的训练迭代轮数可以有不同的上下文词或噪声词。这种方法的好处是什么？尝试实现这种训练方法。
