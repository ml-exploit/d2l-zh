<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>11.2. 凸性 &#8212; 动手学深度学习 2.0.0 documentation</title>

    <link rel="stylesheet" href="../_static/material-design-lite-1.3.0/material.blue-deep_orange.min.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fontawesome/all.css" type="text/css" />
    <link rel="stylesheet" href="../_static/fonts.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../_static/d2l.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/d2l.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="11.3. 梯度下降" href="gd.html" />
    <link rel="prev" title="11.1. 优化和深度学习" href="optimization-intro.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="index.html"><span class="section-number">11. </span>优化算法</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active"><span class="section-number">11.2. </span>凸性</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
          <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
<a id="button-show-source"
    class="mdl-button mdl-js-button mdl-button--icon"
    href="../_sources/chapter_optimization/convexity.rst.txt" rel="nofollow">
  <i class="material-icons">code</i>
</a>
<div class="mdl-tooltip" data-mdl-for="button-show-source">
Show Source
</div>
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          
              <a  class="mdl-navigation__link" href="ml-exploit.github.io/d2l-zh/download/d2l-zh-mlx.pdf">
                  <i class="fas fa-file-pdf"></i>
                  MLX
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf">
                  <i class="fas fa-file-pdf"></i>
                  PyTorch
              </a>
          
              <a  class="mdl-navigation__link" href="https://zh-v2.d2l.ai/d2l-zh.zip">
                  <i class="fas fa-download"></i>
                  Jupyter 记事本
              </a>
          
              <a  class="mdl-navigation__link" href="https://courses.d2l.ai/zh-v2/">
                  <i class="fas fa-user-graduate"></i>
                  课程
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/d2l-ai/d2l-zh">
                  <i class="fab fa-github"></i>
                  GitHub
              </a>
          
              <a  class="mdl-navigation__link" href="https://d2l.ai">
                  <i class="fas fa-external-link-alt"></i>
                  English
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 数据操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. 数据预处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. 线性代数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. 微积分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. 概率</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. 查阅文档</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index.html">3. 线性神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression.html">3.1. 线性回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch.html">3.2. 线性回归的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-concise.html">3.3. 线性回归的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression.html">3.4. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/image-classification-dataset.html">3.5. 图像分类数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch.html">3.6. softmax回归的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-concise.html">3.7. softmax回归的简洁实现</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">4. 多层感知机</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">4.1. 多层感知机</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch.html">4.2. 多层感知机的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-concise.html">4.3. 多层感知机的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit.html">4.4. 模型选择、欠拟合和过拟合</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay.html">4.5. 权重衰减</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">4.6. 暂退法（Dropout）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">4.7. 前向传播、反向传播和计算图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">4.8. 数值稳定性和模型初始化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment.html">4.9. 环境和分布偏移</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">4.10. 实战Kaggle比赛：预测房价</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index.html">5. 深度学习计算</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction.html">5.1. 层和块</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters.html">5.2. 参数管理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer.html">5.3. 自定义层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write.html">5.4. 读写文件</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">6. 卷积神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">6.1. 从全连接层到卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">6.2. 图像卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">6.3. 填充和步幅</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">6.4. 多输入多输出通道</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">6.5. 汇聚层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">6.6. 卷积神经网络（LeNet）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">7. 现代卷积神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">7.1. 深度卷积神经网络（AlexNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">7.2. 使用块的网络（VGG）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">7.3. 网络中的网络（NiN）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">7.4. 含并行连结的网络（GoogLeNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">7.5. 批量规范化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">7.6. 残差网络（ResNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">7.7. 稠密连接网络（DenseNet）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">8. 循环神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">8.1. 序列模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing.html">8.2. 文本预处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">8.3. 语言模型和数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">8.4. 循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">8.5. 循环神经网络的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">8.6. 循环神经网络的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">8.7. 通过时间反向传播</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">9. 现代循环神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">9.1. 门控循环单元（GRU）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">9.2. 长短期记忆网络（LSTM）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">9.3. 深度循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">9.4. 双向循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">9.5. 机器翻译与数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">9.6. 编码器-解码器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">9.7. 序列到序列学习（seq2seq）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">9.8. 束搜索</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index.html">10. 注意力机制</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention-cues.html">10.1. 注意力提示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/nadaraya-waston.html">10.2. 注意力汇聚：Nadaraya-Watson 核回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention-scoring-functions.html">10.3. 注意力评分函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/bahdanau-attention.html">10.4. Bahdanau 注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/multihead-attention.html">10.5. 多头注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">10.6. 自注意力和位置编码</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer.html">10.7. Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. 优化算法</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="optimization-intro.html">11.1. 优化和深度学习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.2. 凸性</a></li>
<li class="toctree-l2"><a class="reference internal" href="gd.html">11.3. 梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgd.html">11.4. 随机梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="minibatch-sgd.html">11.5. 小批量随机梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="momentum.html">11.6. 动量法</a></li>
<li class="toctree-l2"><a class="reference internal" href="adagrad.html">11.7. AdaGrad算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="rmsprop.html">11.8. RMSProp算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="adadelta.html">11.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="adam.html">11.10. Adam算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="lr-scheduler.html">11.11. 学习率调度器</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">12. 计算性能</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">13. 计算机视觉</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">13.1. 图像增广</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">13.2. 微调</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">13.3. 目标检测和边界框</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">13.4. 锚框</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">13.5. 多尺度目标检测</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">13.6. 目标检测数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">13.7. 单发多框检测（SSD）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">13.8. 区域卷积神经网络（R-CNN）系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">13.9. 语义分割和数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">13.10. 转置卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">13.11. 全卷积网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">13.12. 风格迁移</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">14. 自然语言处理：预训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">14.1. 词嵌入（word2vec）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">14.2. 近似训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. 用于预训练词嵌入的数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. 预训练word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">14.5. 全局向量的词嵌入（GloVe）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">14.6. 子词嵌入</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">14.7. 词的相似性和类比任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">14.8. 来自Transformers的双向编码器表示（BERT）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">14.9. 用于预训练BERT的数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">14.10. 预训练BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">15. 自然语言处理：应用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">15.1. 情感分析及数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">15.2. 情感分析：使用循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">15.3. 情感分析：使用卷积神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">15.4. 自然语言推断与数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">15.5. 自然语言推断：使用注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">15.6. 针对序列级和词元级应用微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">15.7. 自然语言推断：微调BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">16. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">16.1. 使用Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">16.2. 使用Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">16.3. 使用Amazon EC2实例</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">16.4. 选择服务器和GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">16.5. 为本书做贡献</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">16.6. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">16.7. <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API 文档</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">

	<script type="text/javascript" src="../_static/sphinx_materialdesign_theme.js "></script>
    <header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../index.html">
              <img class="logo" src="../_static/logo-with-text.png" alt="动手学深度学习"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
            
            <nav class="mdl-navigation">
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preface/index.html">前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_installation/index.html">安装</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_notation/index.html">符号</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../chapter_introduction/index.html">1. 引言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_preliminaries/index.html">2. 预备知识</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/ndarray.html">2.1. 数据操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/pandas.html">2.2. 数据预处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/linear-algebra.html">2.3. 线性代数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/calculus.html">2.4. 微积分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/autograd.html">2.5. 自动微分</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/probability.html">2.6. 概率</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_preliminaries/lookup-api.html">2.7. 查阅文档</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_linear-networks/index.html">3. 线性神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression.html">3.1. 线性回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-scratch.html">3.2. 线性回归的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/linear-regression-concise.html">3.3. 线性回归的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression.html">3.4. softmax回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/image-classification-dataset.html">3.5. 图像分类数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-scratch.html">3.6. softmax回归的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_linear-networks/softmax-regression-concise.html">3.7. softmax回归的简洁实现</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_multilayer-perceptrons/index.html">4. 多层感知机</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp.html">4.1. 多层感知机</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-scratch.html">4.2. 多层感知机的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/mlp-concise.html">4.3. 多层感知机的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/underfit-overfit.html">4.4. 模型选择、欠拟合和过拟合</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay.html">4.5. 权重衰减</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/dropout.html">4.6. 暂退法（Dropout）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/backprop.html">4.7. 前向传播、反向传播和计算图</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/numerical-stability-and-init.html">4.8. 数值稳定性和模型初始化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/environment.html">4.9. 环境和分布偏移</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_multilayer-perceptrons/kaggle-house-price.html">4.10. 实战Kaggle比赛：预测房价</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_deep-learning-computation/index.html">5. 深度学习计算</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/model-construction.html">5.1. 层和块</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/parameters.html">5.2. 参数管理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/custom-layer.html">5.3. 自定义层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_deep-learning-computation/read-write.html">5.4. 读写文件</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-neural-networks/index.html">6. 卷积神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/why-conv.html">6.1. 从全连接层到卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/conv-layer.html">6.2. 图像卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/padding-and-strides.html">6.3. 填充和步幅</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/channels.html">6.4. 多输入多输出通道</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/pooling.html">6.5. 汇聚层</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-neural-networks/lenet.html">6.6. 卷积神经网络（LeNet）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_convolutional-modern/index.html">7. 现代卷积神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/alexnet.html">7.1. 深度卷积神经网络（AlexNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/vgg.html">7.2. 使用块的网络（VGG）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/nin.html">7.3. 网络中的网络（NiN）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/googlenet.html">7.4. 含并行连结的网络（GoogLeNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/batch-norm.html">7.5. 批量规范化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/resnet.html">7.6. 残差网络（ResNet）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_convolutional-modern/densenet.html">7.7. 稠密连接网络（DenseNet）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-neural-networks/index.html">8. 循环神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/sequence.html">8.1. 序列模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/text-preprocessing.html">8.2. 文本预处理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/language-models-and-dataset.html">8.3. 语言模型和数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn.html">8.4. 循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html">8.5. 循环神经网络的从零开始实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-concise.html">8.6. 循环神经网络的简洁实现</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-neural-networks/bptt.html">8.7. 通过时间反向传播</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_recurrent-modern/index.html">9. 现代循环神经网络</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/gru.html">9.1. 门控循环单元（GRU）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/lstm.html">9.2. 长短期记忆网络（LSTM）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/deep-rnn.html">9.3. 深度循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/bi-rnn.html">9.4. 双向循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/machine-translation-and-dataset.html">9.5. 机器翻译与数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/encoder-decoder.html">9.6. 编码器-解码器架构</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/seq2seq.html">9.7. 序列到序列学习（seq2seq）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_recurrent-modern/beam-search.html">9.8. 束搜索</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_attention-mechanisms/index.html">10. 注意力机制</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention-cues.html">10.1. 注意力提示</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/nadaraya-waston.html">10.2. 注意力汇聚：Nadaraya-Watson 核回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/attention-scoring-functions.html">10.3. 注意力评分函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/bahdanau-attention.html">10.4. Bahdanau 注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/multihead-attention.html">10.5. 多头注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/self-attention-and-positional-encoding.html">10.6. 自注意力和位置编码</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_attention-mechanisms/transformer.html">10.7. Transformer</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">11. 优化算法</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="optimization-intro.html">11.1. 优化和深度学习</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">11.2. 凸性</a></li>
<li class="toctree-l2"><a class="reference internal" href="gd.html">11.3. 梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="sgd.html">11.4. 随机梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="minibatch-sgd.html">11.5. 小批量随机梯度下降</a></li>
<li class="toctree-l2"><a class="reference internal" href="momentum.html">11.6. 动量法</a></li>
<li class="toctree-l2"><a class="reference internal" href="adagrad.html">11.7. AdaGrad算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="rmsprop.html">11.8. RMSProp算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="adadelta.html">11.9. Adadelta</a></li>
<li class="toctree-l2"><a class="reference internal" href="adam.html">11.10. Adam算法</a></li>
<li class="toctree-l2"><a class="reference internal" href="lr-scheduler.html">11.11. 学习率调度器</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computational-performance/index.html">12. 计算性能</a><ul class="simple">
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_computer-vision/index.html">13. 计算机视觉</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/image-augmentation.html">13.1. 图像增广</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fine-tuning.html">13.2. 微调</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/bounding-box.html">13.3. 目标检测和边界框</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/anchor.html">13.4. 锚框</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/multiscale-object-detection.html">13.5. 多尺度目标检测</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/object-detection-dataset.html">13.6. 目标检测数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/ssd.html">13.7. 单发多框检测（SSD）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/rcnn.html">13.8. 区域卷积神经网络（R-CNN）系列</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/semantic-segmentation-and-dataset.html">13.9. 语义分割和数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/transposed-conv.html">13.10. 转置卷积</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/fcn.html">13.11. 全卷积网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/neural-style.html">13.12. 风格迁移</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-cifar10.html">13.13. 实战 Kaggle 比赛：图像分类 (CIFAR-10)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_computer-vision/kaggle-dog.html">13.14. 实战Kaggle比赛：狗的品种识别（ImageNet Dogs）</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/index.html">14. 自然语言处理：预训练</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec.html">14.1. 词嵌入（word2vec）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/approx-training.html">14.2. 近似训练</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word-embedding-dataset.html">14.3. 用于预训练词嵌入的数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/word2vec-pretraining.html">14.4. 预训练word2vec</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/glove.html">14.5. 全局向量的词嵌入（GloVe）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/subword-embedding.html">14.6. 子词嵌入</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/similarity-analogy.html">14.7. 词的相似性和类比任务</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert.html">14.8. 来自Transformers的双向编码器表示（BERT）</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-dataset.html">14.9. 用于预训练BERT的数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-pretraining/bert-pretraining.html">14.10. 预训练BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_natural-language-processing-applications/index.html">15. 自然语言处理：应用</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-and-dataset.html">15.1. 情感分析及数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-rnn.html">15.2. 情感分析：使用循环神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/sentiment-analysis-cnn.html">15.3. 情感分析：使用卷积神经网络</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-and-dataset.html">15.4. 自然语言推断与数据集</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-attention.html">15.5. 自然语言推断：使用注意力</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/finetuning-bert.html">15.6. 针对序列级和词元级应用微调BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_natural-language-processing-applications/natural-language-inference-bert.html">15.7. 自然语言推断：微调BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/index.html">16. Appendix: Tools for Deep Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/jupyter.html">16.1. 使用Jupyter Notebook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/sagemaker.html">16.2. 使用Amazon SageMaker</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/aws.html">16.3. 使用Amazon EC2实例</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/selecting-servers-gpus.html">16.4. 选择服务器和GPU</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/contributing.html">16.5. 为本书做贡献</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/utils.html">16.6. Utility Functions and Classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../chapter_appendix-tools-for-deep-learning/d2l.html">16.7. <code class="docutils literal notranslate"><span class="pre">d2l</span></code> API 文档</a></li>
</ul>
</li>
</ul>

            </nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content" role="main">
        
  <div class="section" id="sec-convexity">
<span id="id1"></span><h1><span class="section-number">11.2. </span>凸性<a class="headerlink" href="#sec-convexity" title="Permalink to this heading">¶</a></h1>
<p><em>凸性</em>（convexity）在优化算法的设计中起到至关重要的作用，
这主要是由于在这种情况下对算法进行分析和测试要容易。
换言之，如果算法在凸性条件设定下的效果很差，
那通常我们很难在其他条件下看到好的结果。
此外，即使深度学习中的优化问题通常是非凸的，
它们也经常在局部极小值附近表现出一些凸性。 这可能会产生一些像
<span id="id2">()</span>这样比较有意思的新优化变体。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">mlx.core</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">mx</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits</span><span class="w"> </span><span class="kn">import</span> <span class="n">mplot3d</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">d2l</span><span class="w"> </span><span class="kn">import</span> <span class="n">mlx</span> <span class="k">as</span> <span class="n">d2l</span>
</pre></div>
</div>
<div class="section" id="id3">
<h2><span class="section-number">11.2.1. </span>定义<a class="headerlink" href="#id3" title="Permalink to this heading">¶</a></h2>
<p>在进行凸分析之前，我们需要定义<em>凸集</em>（convex
sets）和<em>凸函数</em>（convex functions）。</p>
<div class="section" id="id4">
<h3><span class="section-number">11.2.1.1. </span>凸集<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<p><em>凸集</em>（convex set）是凸性的基础。
简单地说，如果对于任何<span class="math notranslate nohighlight">\(a, b \in \mathcal{X}\)</span>，连接<span class="math notranslate nohighlight">\(a\)</span>和<span class="math notranslate nohighlight">\(b\)</span>的线段也位于<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>中，则向量空间中的一个集合<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>是<em>凸</em>（convex）的。
在数学术语上，这意味着对于所有<span class="math notranslate nohighlight">\(\lambda \in [0, 1]\)</span>，我们得到</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-0">
<span class="eqno">(11.2.1)<a class="headerlink" href="#equation-chapter-optimization-convexity-0" title="Permalink to this equation">¶</a></span>\[\lambda  a + (1-\lambda)  b \in \mathcal{X} \text{ 当 } a, b \in \mathcal{X}.\]</div>
<p>这听起来有点抽象，那我们来看一下 <a class="reference internal" href="#fig-pacman"><span class="std std-numref">图11.2.1</span></a>里的例子。
第一组存在不包含在集合内部的线段，所以该集合是非凸的，而另外两组则没有这样的问题。</p>
<div class="figure align-default" id="id17">
<span id="fig-pacman"></span><img alt="../_images/pacman.svg" src="../_images/pacman.svg" /><p class="caption"><span class="caption-number">图11.2.1 </span><span class="caption-text">第一组是非凸的，另外两组是凸的。</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>接下来来看一下交集 <a class="reference internal" href="#fig-convex-intersect"><span class="std std-numref">图11.2.2</span></a>。
假设<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>和<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>是凸集，那么<span class="math notranslate nohighlight">\(\mathcal {X} \cap \mathcal{Y}\)</span>也是凸集的。
现在考虑任意<span class="math notranslate nohighlight">\(a, b \in \mathcal{X} \cap \mathcal{Y}\)</span>，
因为<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>和<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>是凸集，
所以连接<span class="math notranslate nohighlight">\(a\)</span>和<span class="math notranslate nohighlight">\(b\)</span>的线段包含在<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>和<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>中。
鉴于此，它们也需要包含在<span class="math notranslate nohighlight">\(\mathcal {X} \cap \mathcal{Y}\)</span>中，从而证明我们的定理。</p>
<div class="figure align-default" id="id18">
<span id="fig-convex-intersect"></span><img alt="../_images/convex-intersect.svg" src="../_images/convex-intersect.svg" /><p class="caption"><span class="caption-number">图11.2.2 </span><span class="caption-text">两个凸集的交集是凸的。</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>我们可以毫不费力地进一步得到这样的结果：
给定凸集<span class="math notranslate nohighlight">\(\mathcal{X}_i\)</span>，它们的交集<span class="math notranslate nohighlight">\(\cap_{i} \mathcal{X}_i\)</span>是凸的。
但是反向是不正确的，考虑两个不相交的集合<span class="math notranslate nohighlight">\(\mathcal{X} \cap \mathcal{Y} = \emptyset\)</span>，
取<span class="math notranslate nohighlight">\(a \in \mathcal{X}\)</span>和<span class="math notranslate nohighlight">\(b \in \mathcal{Y}\)</span>。
因为我们假设<span class="math notranslate nohighlight">\(\mathcal{X} \cap \mathcal{Y} = \emptyset\)</span>， 在
<a class="reference internal" href="#fig-nonconvex"><span class="std std-numref">图11.2.3</span></a>中连接<span class="math notranslate nohighlight">\(a\)</span>和<span class="math notranslate nohighlight">\(b\)</span>的线段需要包含一部分既不在<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>也不在<span class="math notranslate nohighlight">\(\mathcal{Y}\)</span>中。
因此线段也不在<span class="math notranslate nohighlight">\(\mathcal{X} \cup \mathcal{Y}\)</span>中，因此证明了凸集的并集不一定是凸的，即<em>非凸</em>（nonconvex）的。</p>
<div class="figure align-default" id="id19">
<span id="fig-nonconvex"></span><img alt="../_images/nonconvex.svg" src="../_images/nonconvex.svg" /><p class="caption"><span class="caption-number">图11.2.3 </span><span class="caption-text">两个凸集的并集不一定是凸的。</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>通常，深度学习中的问题是在凸集上定义的。
例如，<span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>，即实数的<span class="math notranslate nohighlight">\(d\)</span>-维向量的集合是凸集（毕竟<span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>中任意两点之间的线存在<span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>）中。
在某些情况下，我们使用有界长度的变量，例如球的半径定义为<span class="math notranslate nohighlight">\(\{\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \text{ 且 } \| \mathbf{x} \| \leq r\}\)</span>。</p>
</div>
<div class="section" id="id5">
<h3><span class="section-number">11.2.1.2. </span>凸函数<a class="headerlink" href="#id5" title="Permalink to this heading">¶</a></h3>
<p>现在我们有了凸集，我们可以引入<em>凸函数</em>（convex
function）<span class="math notranslate nohighlight">\(f\)</span>。
给定一个凸集<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>，如果对于所有<span class="math notranslate nohighlight">\(x, x' \in \mathcal{X}\)</span>和所有<span class="math notranslate nohighlight">\(\lambda \in [0, 1]\)</span>，函数<span class="math notranslate nohighlight">\(f: \mathcal{X} \to \mathbb{R}\)</span>是凸的，我们可以得到</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-1">
<span class="eqno">(11.2.2)<a class="headerlink" href="#equation-chapter-optimization-convexity-1" title="Permalink to this equation">¶</a></span>\[\lambda f(x) + (1-\lambda) f(x') \geq f(\lambda x + (1-\lambda) x').\]</div>
<p>为了说明这一点，让我们绘制一些函数并检查哪些函数满足要求。
下面我们定义一些函数，包括凸函数和非凸函数。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># 凸函数</span>
<span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mx</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 非凸函数</span>
<span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">mx</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># 凸函数</span>

<span class="n">x</span><span class="p">,</span> <span class="n">segment</span> <span class="o">=</span> <span class="n">mx</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">mx</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">use_svg_display</span><span class="p">()</span>
<span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">d2l</span><span class="o">.</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="n">f</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">h</span><span class="p">]):</span>
    <span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">segment</span><span class="p">],</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">func</span><span class="p">(</span><span class="n">segment</span><span class="p">)],</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_convexity_6bcbdc_3_0.svg" src="../_images/output_convexity_6bcbdc_3_0.svg" /></div>
<p>不出所料，余弦函数为非凸的，而抛物线函数和指数函数为凸的。
请注意，为使该条件有意义，<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>是凸集的要求是必要的。
否则可能无法很好地界定<span class="math notranslate nohighlight">\(f(\lambda x + (1-\lambda) x')\)</span>的结果。</p>
</div>
<div class="section" id="id6">
<h3><span class="section-number">11.2.1.3. </span>詹森不等式<a class="headerlink" href="#id6" title="Permalink to this heading">¶</a></h3>
<p>给定一个凸函数<span class="math notranslate nohighlight">\(f\)</span>，最有用的数学工具之一就是<em>詹森不等式</em>（Jensen’s
inequality）。 它是凸性定义的一种推广：</p>
<div class="math notranslate nohighlight" id="equation-eq-jensens-inequality">
<span class="eqno">(11.2.3)<a class="headerlink" href="#equation-eq-jensens-inequality" title="Permalink to this equation">¶</a></span>\[\sum_i \alpha_i f(x_i)  \geq f\left(\sum_i \alpha_i x_i\right) \text{ and } E_X[f(X)] \geq f\left(E_X[X]\right),\]</div>
<p>其中<span class="math notranslate nohighlight">\(\alpha_i\)</span>是满足<span class="math notranslate nohighlight">\(\sum_i \alpha_i = 1\)</span>的非负实数，<span class="math notranslate nohighlight">\(X\)</span>是随机变量。
换句话说，凸函数的期望不小于期望的凸函数，其中后者通常是一个更简单的表达式。
为了证明第一个不等式，我们多次将凸性的定义应用于一次求和中的一项。</p>
<p>詹森不等式的一个常见应用：用一个较简单的表达式约束一个较复杂的表达式。
例如，它可以应用于部分观察到的随机变量的对数似然。
具体地说，由于<span class="math notranslate nohighlight">\(\int P(Y) P(X \mid Y) dY = P(X)\)</span>，所以</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-2">
<span class="eqno">(11.2.4)<a class="headerlink" href="#equation-chapter-optimization-convexity-2" title="Permalink to this equation">¶</a></span>\[E_{Y \sim P(Y)}[-\log P(X \mid Y)] \geq -\log P(X),\]</div>
<p>这里，<span class="math notranslate nohighlight">\(Y\)</span>是典型的未观察到的随机变量，<span class="math notranslate nohighlight">\(P(Y)\)</span>是它可能如何分布的最佳猜测，<span class="math notranslate nohighlight">\(P(X)\)</span>是将<span class="math notranslate nohighlight">\(Y\)</span>积分后的分布。
例如，在聚类中<span class="math notranslate nohighlight">\(Y\)</span>可能是簇标签，而在应用簇标签时，<span class="math notranslate nohighlight">\(P(X \mid Y)\)</span>是生成模型。</p>
</div>
</div>
<div class="section" id="id7">
<h2><span class="section-number">11.2.2. </span>性质<a class="headerlink" href="#id7" title="Permalink to this heading">¶</a></h2>
<p>下面我们来看一下凸函数一些有趣的性质。</p>
<div class="section" id="id8">
<h3><span class="section-number">11.2.2.1. </span>局部极小值是全局极小值<a class="headerlink" href="#id8" title="Permalink to this heading">¶</a></h3>
<p>首先凸函数的局部极小值也是全局极小值。 下面我们用反证法给出证明。</p>
<p>假设<span class="math notranslate nohighlight">\(x^{\ast} \in \mathcal{X}\)</span>是一个局部最小值，则存在一个很小的正值<span class="math notranslate nohighlight">\(p\)</span>，使得当<span class="math notranslate nohighlight">\(x \in \mathcal{X}\)</span>满足<span class="math notranslate nohighlight">\(0 &lt; |x - x^{\ast}| \leq p\)</span>时，有<span class="math notranslate nohighlight">\(f(x^{\ast}) &lt; f(x)\)</span>。</p>
<p>现在假设局部极小值<span class="math notranslate nohighlight">\(x^{\ast}\)</span>不是<span class="math notranslate nohighlight">\(f\)</span>的全局极小值：存在<span class="math notranslate nohighlight">\(x' \in \mathcal{X}\)</span>使得<span class="math notranslate nohighlight">\(f(x') &lt; f(x^{\ast})\)</span>。
则存在
<span class="math notranslate nohighlight">\(\lambda \in [0, 1)\)</span>，比如<span class="math notranslate nohighlight">\(\lambda = 1 - \frac{p}{|x^{\ast} - x'|}\)</span>，使得
<span class="math notranslate nohighlight">\(0 &lt; |\lambda x^{\ast} + (1-\lambda) x' - x^{\ast}| \leq p\)</span>。</p>
<p>然而，根据凸性的性质，有</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-3">
<span class="eqno">(11.2.5)<a class="headerlink" href="#equation-chapter-optimization-convexity-3" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned}
    f(\lambda x^{\ast} + (1-\lambda) x') &amp;\leq \lambda f(x^{\ast}) + (1-\lambda) f(x') \\
    &amp;&lt; \lambda f(x^{\ast}) + (1-\lambda) f(x^{\ast}) \\
    &amp;= f(x^{\ast}), \\
\end{aligned}\end{split}\]</div>
<p>这与<span class="math notranslate nohighlight">\(x^{\ast}\)</span>是局部最小值相矛盾。
因此，不存在<span class="math notranslate nohighlight">\(x' \in \mathcal{X}\)</span>满足<span class="math notranslate nohighlight">\(f(x') &lt; f(x^{\ast})\)</span>。
综上所述，局部最小值<span class="math notranslate nohighlight">\(x^{\ast}\)</span>也是全局最小值。</p>
<p>例如，对于凸函数<span class="math notranslate nohighlight">\(f(x) = (x-1)^2\)</span>，有一个局部最小值<span class="math notranslate nohighlight">\(x=1\)</span>，它也是全局最小值。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">set_figsize</span><span class="p">()</span>
<span class="n">d2l</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">segment</span><span class="p">],</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">f</span><span class="p">(</span><span class="n">segment</span><span class="p">)],</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure align-default">
<img alt="../_images/output_convexity_6bcbdc_5_0.svg" src="../_images/output_convexity_6bcbdc_5_0.svg" /></div>
<p>凸函数的局部极小值同时也是全局极小值这一性质是很方便的。
这意味着如果我们最小化函数，我们就不会“卡住”。
但是请注意，这并不意味着不能有多个全局最小值，或者可能不存在一个全局最小值。
例如，函数<span class="math notranslate nohighlight">\(f(x) = \mathrm{max}(|x|-1, 0)\)</span>在<span class="math notranslate nohighlight">\([-1,1]\)</span>区间上都是最小值。
相反，函数<span class="math notranslate nohighlight">\(f(x) = \exp(x)\)</span>在<span class="math notranslate nohighlight">\(\mathbb{R}\)</span>上没有取得最小值。对于<span class="math notranslate nohighlight">\(x \to -\infty\)</span>，它趋近于<span class="math notranslate nohighlight">\(0\)</span>，但是没有<span class="math notranslate nohighlight">\(f(x) = 0\)</span>的<span class="math notranslate nohighlight">\(x\)</span>。</p>
</div>
<div class="section" id="id9">
<h3><span class="section-number">11.2.2.2. </span>凸函数的下水平集是凸的<a class="headerlink" href="#id9" title="Permalink to this heading">¶</a></h3>
<p>我们可以方便地通过凸函数的<em>下水平集</em>（below sets）定义凸集。
具体来说，给定一个定义在凸集<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>上的凸函数<span class="math notranslate nohighlight">\(f\)</span>，其任意一个下水平集</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-4">
<span class="eqno">(11.2.6)<a class="headerlink" href="#equation-chapter-optimization-convexity-4" title="Permalink to this equation">¶</a></span>\[\mathcal{S}_b := \{x | x \in \mathcal{X} \text{ and } f(x) \leq b\}\]</div>
<p>是凸的。</p>
<p>让我们快速证明一下。
对于任何<span class="math notranslate nohighlight">\(x, x' \in \mathcal{S}_b\)</span>，我们需要证明：当<span class="math notranslate nohighlight">\(\lambda \in [0, 1]\)</span>时，<span class="math notranslate nohighlight">\(\lambda x + (1-\lambda) x' \in \mathcal{S}_b\)</span>。
因为<span class="math notranslate nohighlight">\(f(x) \leq b\)</span>且<span class="math notranslate nohighlight">\(f(x') \leq b\)</span>，所以</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-5">
<span class="eqno">(11.2.7)<a class="headerlink" href="#equation-chapter-optimization-convexity-5" title="Permalink to this equation">¶</a></span>\[f(\lambda x + (1-\lambda) x') \leq \lambda f(x) + (1-\lambda) f(x') \leq b.\]</div>
</div>
<div class="section" id="id10">
<h3><span class="section-number">11.2.2.3. </span>凸性和二阶导数<a class="headerlink" href="#id10" title="Permalink to this heading">¶</a></h3>
<p>当一个函数的二阶导数<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>存在时，我们很容易检查这个函数的凸性。
我们需要做的就是检查<span class="math notranslate nohighlight">\(\nabla^2f \succeq 0\)</span>，
即对于所有<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^n\)</span>，<span class="math notranslate nohighlight">\(\mathbf{x}^\top \mathbf{H} \mathbf{x} \geq 0\)</span>.
例如，函数<span class="math notranslate nohighlight">\(f(\mathbf{x}) = \frac{1}{2} \|\mathbf{x}\|^2\)</span>是凸的，因为<span class="math notranslate nohighlight">\(\nabla^2 f = \mathbf{1}\)</span>，即其导数是单位矩阵。</p>
<p>更正式地讲，<span class="math notranslate nohighlight">\(f\)</span>为凸函数，当且仅当任意二次可微一维函数<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>是凸的。
对于任意二次可微多维函数<span class="math notranslate nohighlight">\(f: \mathbb{R}^{n} \rightarrow \mathbb{R}\)</span>，
它是凸的当且仅当它的Hessian<span class="math notranslate nohighlight">\(\nabla^2f\succeq 0\)</span>。</p>
<p>首先，我们来证明一下一维情况。
为了证明凸函数的<span class="math notranslate nohighlight">\(f''(x) \geq 0\)</span>，我们使用：</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-6">
<span class="eqno">(11.2.8)<a class="headerlink" href="#equation-chapter-optimization-convexity-6" title="Permalink to this equation">¶</a></span>\[\frac{1}{2} f(x + \epsilon) + \frac{1}{2} f(x - \epsilon) \geq f\left(\frac{x + \epsilon}{2} + \frac{x - \epsilon}{2}\right) = f(x).\]</div>
<p>因为二阶导数是由有限差分的极限给出的，所以遵循</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-7">
<span class="eqno">(11.2.9)<a class="headerlink" href="#equation-chapter-optimization-convexity-7" title="Permalink to this equation">¶</a></span>\[f''(x) = \lim_{\epsilon \to 0} \frac{f(x+\epsilon) + f(x - \epsilon) - 2f(x)}{\epsilon^2} \geq 0.\]</div>
<p>为了证明<span class="math notranslate nohighlight">\(f'' \geq 0\)</span>可以推导<span class="math notranslate nohighlight">\(f\)</span>是凸的，
我们使用这样一个事实：<span class="math notranslate nohighlight">\(f'' \geq 0\)</span>意味着<span class="math notranslate nohighlight">\(f'\)</span>是一个单调的非递减函数。
假设<span class="math notranslate nohighlight">\(a &lt; x &lt; b\)</span>是<span class="math notranslate nohighlight">\(\mathbb{R}\)</span>中的三个点，
其中，<span class="math notranslate nohighlight">\(x = (1-\lambda)a + \lambda b\)</span>且<span class="math notranslate nohighlight">\(\lambda \in (0, 1)\)</span>.
根据中值定理，存在<span class="math notranslate nohighlight">\(\alpha \in [a, x]\)</span>，<span class="math notranslate nohighlight">\(\beta \in [x, b]\)</span>，使得</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-8">
<span class="eqno">(11.2.10)<a class="headerlink" href="#equation-chapter-optimization-convexity-8" title="Permalink to this equation">¶</a></span>\[f'(\alpha) = \frac{f(x) - f(a)}{x-a} \text{ 且 } f'(\beta) = \frac{f(b) - f(x)}{b-x}.\]</div>
<p>通过单调性<span class="math notranslate nohighlight">\(f'(\beta) \geq f'(\alpha)\)</span>，因此</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-9">
<span class="eqno">(11.2.11)<a class="headerlink" href="#equation-chapter-optimization-convexity-9" title="Permalink to this equation">¶</a></span>\[\frac{x-a}{b-a}f(b) + \frac{b-x}{b-a}f(a) \geq f(x).\]</div>
<p>由于<span class="math notranslate nohighlight">\(x = (1-\lambda)a + \lambda b\)</span>，所以</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-10">
<span class="eqno">(11.2.12)<a class="headerlink" href="#equation-chapter-optimization-convexity-10" title="Permalink to this equation">¶</a></span>\[\lambda f(b) + (1-\lambda)f(a) \geq f((1-\lambda)a + \lambda b),\]</div>
<p>从而证明了凸性。</p>
<p>第二，我们需要一个引理证明多维情况：
<span class="math notranslate nohighlight">\(f: \mathbb{R}^n \rightarrow \mathbb{R}\)</span>
是凸的当且仅当对于所有<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span></p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-11">
<span class="eqno">(11.2.13)<a class="headerlink" href="#equation-chapter-optimization-convexity-11" title="Permalink to this equation">¶</a></span>\[g(z) \stackrel{\mathrm{def}}{=} f(z \mathbf{x} + (1-z)  \mathbf{y}) \text{ where } z \in [0,1]\]</div>
<p>是凸的。</p>
<p>为了证明<span class="math notranslate nohighlight">\(f\)</span>的凸性意味着<span class="math notranslate nohighlight">\(g\)</span>是凸的，
我们可以证明，对于所有的<span class="math notranslate nohighlight">\(a，b，\lambda \in[0，1]\)</span>（这样有<span class="math notranslate nohighlight">\(0 \leq \lambda a + (1-\lambda) b \leq 1\)</span>），</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-12">
<span class="eqno">(11.2.14)<a class="headerlink" href="#equation-chapter-optimization-convexity-12" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} &amp;g(\lambda a + (1-\lambda) b)\\
=&amp;f\left(\left(\lambda a + (1-\lambda) b\right)\mathbf{x} + \left(1-\lambda a - (1-\lambda) b\right)\mathbf{y} \right)\\
=&amp;f\left(\lambda \left(a \mathbf{x} + (1-a)  \mathbf{y}\right)  + (1-\lambda) \left(b \mathbf{x} + (1-b)  \mathbf{y}\right) \right)\\
\leq&amp; \lambda f\left(a \mathbf{x} + (1-a)  \mathbf{y}\right)  + (1-\lambda) f\left(b \mathbf{x} + (1-b)  \mathbf{y}\right) \\
=&amp; \lambda g(a) + (1-\lambda) g(b).
\end{aligned}\end{split}\]</div>
<p>为了证明这一点，我们可以证明对
<span class="math notranslate nohighlight">\([0，1]\)</span>中所有的<span class="math notranslate nohighlight">\(\lambda\)</span>：</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-13">
<span class="eqno">(11.2.15)<a class="headerlink" href="#equation-chapter-optimization-convexity-13" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} &amp;f(\lambda \mathbf{x} + (1-\lambda) \mathbf{y})\\
=&amp;g(\lambda \cdot 1 + (1-\lambda) \cdot 0)\\
\leq&amp; \lambda g(1)  + (1-\lambda) g(0) \\
=&amp; \lambda f(\mathbf{x}) + (1-\lambda) f(\mathbf{y}).
\end{aligned}\end{split}\]</div>
<p>最后，利用上面的引理和一维情况的结果，我们可以证明多维情况：
多维函数<span class="math notranslate nohighlight">\(f:\mathbb{R}^n\rightarrow\mathbb{R}\)</span>是凸函数，当且仅当<span class="math notranslate nohighlight">\(g(z) \stackrel{\mathrm{def}}{=} f(z \mathbf{x} + (1-z) \mathbf{y})\)</span>是凸的，这里<span class="math notranslate nohighlight">\(z \in [0,1]\)</span>，<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span>。
根据一维情况，
此条成立的条件为，当且仅当对于所有<span class="math notranslate nohighlight">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span>，
<span class="math notranslate nohighlight">\(g'' = (\mathbf{x} - \mathbf{y})^\top \mathbf{H}(\mathbf{x} - \mathbf{y}) \geq 0\)</span>（<span class="math notranslate nohighlight">\(\mathbf{H} \stackrel{\mathrm{def}}{=} \nabla^2f\)</span>）。
这相当于根据半正定矩阵的定义，<span class="math notranslate nohighlight">\(\mathbf{H} \succeq 0\)</span>。</p>
</div>
</div>
<div class="section" id="id11">
<h2><span class="section-number">11.2.3. </span>约束<a class="headerlink" href="#id11" title="Permalink to this heading">¶</a></h2>
<p>凸优化的一个很好的特性是能够让我们有效地处理<em>约束</em>（constraints）。
即它使我们能够解决以下形式的<em>约束优化</em>（constrained
optimization）问题：</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-14">
<span class="eqno">(11.2.16)<a class="headerlink" href="#equation-chapter-optimization-convexity-14" title="Permalink to this equation">¶</a></span>\[\begin{split}\begin{aligned} \mathop{\mathrm{minimize~}}_{\mathbf{x}} &amp; f(\mathbf{x}) \\
    \text{ subject to } &amp; c_i(\mathbf{x}) \leq 0 \text{ for all } i \in \{1, \ldots, N\}.
\end{aligned}\end{split}\]</div>
<p>这里<span class="math notranslate nohighlight">\(f\)</span>是目标函数，<span class="math notranslate nohighlight">\(c_i\)</span>是约束函数。
例如第一个约束<span class="math notranslate nohighlight">\(c_1(\mathbf{x}) = \|\mathbf{x}\|_2 - 1\)</span>，则参数<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>被限制为单位球。
如果第二个约束<span class="math notranslate nohighlight">\(c_2(\mathbf{x}) = \mathbf{v}^\top \mathbf{x} + b\)</span>，那么这对应于半空间上所有的<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>。
同时满足这两个约束等于选择一个球的切片作为约束集。</p>
<div class="section" id="id12">
<h3><span class="section-number">11.2.3.1. </span>拉格朗日函数<a class="headerlink" href="#id12" title="Permalink to this heading">¶</a></h3>
<p>通常，求解一个有约束的优化问题是困难的，解决这个问题的一种方法来自物理中相当简单的直觉。
想象一个球在一个盒子里，球会滚到最低的地方，重力将与盒子两侧对球施加的力平衡。
简而言之，目标函数（即重力）的梯度将被约束函数的梯度所抵消（由于墙壁的“推回”作用，需要保持在盒子内）。
请注意，任何不起作用的约束（即球不接触壁）都将无法对球施加任何力。</p>
<p>这里我们简略拉格朗日函数<span class="math notranslate nohighlight">\(L\)</span>的推导，上述推理可以通过以下鞍点优化问题来表示：</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-15">
<span class="eqno">(11.2.17)<a class="headerlink" href="#equation-chapter-optimization-convexity-15" title="Permalink to this equation">¶</a></span>\[L(\mathbf{x}, \alpha_1, \ldots, \alpha_n) = f(\mathbf{x}) + \sum_{i=1}^n \alpha_i c_i(\mathbf{x}) \text{ where } \alpha_i \geq 0.\]</div>
<p>这里的变量<span class="math notranslate nohighlight">\(\alpha_i\)</span>（<span class="math notranslate nohighlight">\(i=1,\ldots,n\)</span>）是所谓的<em>拉格朗日乘数</em>（Lagrange
multipliers），它确保约束被正确地执行。
选择它们的大小足以确保所有<span class="math notranslate nohighlight">\(i\)</span>的<span class="math notranslate nohighlight">\(c_i(\mathbf{x}) \leq 0\)</span>。
例如，对于<span class="math notranslate nohighlight">\(c_i(\mathbf{x}) &lt; 0\)</span>中任意<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>，我们最终会选择<span class="math notranslate nohighlight">\(\alpha_i = 0\)</span>。
此外，这是一个<em>鞍点</em>（saddlepoint）优化问题。
在这个问题中，我们想要使<span class="math notranslate nohighlight">\(L\)</span>相对于<span class="math notranslate nohighlight">\(\alpha_i\)</span><em>最大化</em>（maximize），同时使它相对于<span class="math notranslate nohighlight">\(\mathbf{x}\)</span><em>最小化</em>（minimize）。
有大量的文献解释如何得出函数<span class="math notranslate nohighlight">\(L(\mathbf{x}, \alpha_1, \ldots, \alpha_n)\)</span>。
我们这里只需要知道<span class="math notranslate nohighlight">\(L\)</span>的鞍点是原始约束优化问题的最优解就足够了。</p>
</div>
<div class="section" id="id13">
<h3><span class="section-number">11.2.3.2. </span>惩罚<a class="headerlink" href="#id13" title="Permalink to this heading">¶</a></h3>
<p>一种至少近似地满足约束优化问题的方法是采用拉格朗日函数<span class="math notranslate nohighlight">\(L\)</span>。除了满足<span class="math notranslate nohighlight">\(c_i(\mathbf{x}) \leq 0\)</span>之外，我们只需将<span class="math notranslate nohighlight">\(\alpha_i c_i(\mathbf{x})\)</span>添加到目标函数<span class="math notranslate nohighlight">\(f(x)\)</span>。
这样可以确保不会严重违反约束。</p>
<p>事实上，我们一直在使用这个技巧。 比如权重衰减
<a class="reference internal" href="../chapter_multilayer-perceptrons/weight-decay.html#sec-weight-decay"><span class="std std-numref">4.5节</span></a>，在目标函数中加入<span class="math notranslate nohighlight">\(\frac{\lambda}{2} |\mathbf{w}|^2\)</span>，以确保<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>不会增长太大。
使用约束优化的观点，我们可以看到，对于若干半径<span class="math notranslate nohighlight">\(r\)</span>，这将确保<span class="math notranslate nohighlight">\(|\mathbf{w}|^2 - r^2 \leq 0\)</span>。
通过调整<span class="math notranslate nohighlight">\(\lambda\)</span>的值，我们可以改变<span class="math notranslate nohighlight">\(\mathbf{w}\)</span>的大小。</p>
<p>通常，添加惩罚是确保近似满足约束的一种好方法。
在实践中，这被证明比精确的满意度更可靠。
此外，对于非凸问题，许多使精确方法在凸情况下的性质（例如，可求最优解）不再成立。</p>
</div>
<div class="section" id="id14">
<h3><span class="section-number">11.2.3.3. </span>投影<a class="headerlink" href="#id14" title="Permalink to this heading">¶</a></h3>
<p>满足约束条件的另一种策略是<em>投影</em>（projections）。
同样，我们之前也遇到过，例如在
<a class="reference internal" href="../chapter_recurrent-neural-networks/rnn-scratch.html#sec-rnn-scratch"><span class="std std-numref">8.5节</span></a>中处理梯度截断时，我们通过</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-16">
<span class="eqno">(11.2.18)<a class="headerlink" href="#equation-chapter-optimization-convexity-16" title="Permalink to this equation">¶</a></span>\[\mathbf{g} \leftarrow \mathbf{g} \cdot \mathrm{min}(1, \theta/\|\mathbf{g}\|),\]</div>
<p>确保梯度的长度以<span class="math notranslate nohighlight">\(\theta\)</span>为界限。</p>
<p>这就是<span class="math notranslate nohighlight">\(\mathbf{g}\)</span>在半径为<span class="math notranslate nohighlight">\(\theta\)</span>的球上的<em>投影</em>（projection）。
更泛化地说，在凸集<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>上的投影被定义为</p>
<div class="math notranslate nohighlight" id="equation-chapter-optimization-convexity-17">
<span class="eqno">(11.2.19)<a class="headerlink" href="#equation-chapter-optimization-convexity-17" title="Permalink to this equation">¶</a></span>\[\mathrm{Proj}_\mathcal{X}(\mathbf{x}) = \mathop{\mathrm{argmin}}_{\mathbf{x}' \in \mathcal{X}} \|\mathbf{x} - \mathbf{x}'\|.\]</div>
<p>它是<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>中离<span class="math notranslate nohighlight">\(\mathbf{X}\)</span>最近的点。</p>
<div class="figure align-default" id="id20">
<span id="fig-projections"></span><img alt="../_images/projections.svg" src="../_images/projections.svg" /><p class="caption"><span class="caption-number">图11.2.4 </span><span class="caption-text">Convex Projections.</span><a class="headerlink" href="#id20" title="Permalink to this image">¶</a></p>
</div>
<p>投影的数学定义听起来可能有点抽象，为了解释得更清楚一些，请看
<a class="reference internal" href="#fig-projections"><span class="std std-numref">图11.2.4</span></a>。 图中有两个凸集，一个圆和一个菱形。
两个集合内的点（黄色）在投影期间保持不变。
两个集合（黑色）之外的点投影到集合中接近原始点（黑色）的点（红色）。
虽然对<span class="math notranslate nohighlight">\(L_2\)</span>的球面来说，方向保持不变，但一般情况下不需要这样。</p>
<p>凸投影的一个用途是计算稀疏权重向量。
在本例中，我们将权重向量投影到一个<span class="math notranslate nohighlight">\(L_1\)</span>的球上， 这是
<a class="reference internal" href="#fig-projections"><span class="std std-numref">图11.2.4</span></a>中菱形例子的一个广义版本。</p>
</div>
</div>
<div class="section" id="id15">
<h2><span class="section-number">11.2.4. </span>小结<a class="headerlink" href="#id15" title="Permalink to this heading">¶</a></h2>
<p>在深度学习的背景下，凸函数的主要目的是帮助我们详细了解优化算法。
我们由此得出梯度下降法和随机梯度下降法是如何相应推导出来的。</p>
<ul class="simple">
<li><p>凸集的交点是凸的，并集不是。</p></li>
<li><p>根据詹森不等式，“一个多变量凸函数的总期望值”大于或等于“用每个变量的期望值计算这个函数的总值“。</p></li>
<li><p>一个二次可微函数是凸函数，当且仅当其Hessian（二阶导数矩阵）是半正定的。</p></li>
<li><p>凸约束可以通过拉格朗日函数来添加。在实践中，只需在目标函数中加上一个惩罚就可以了。</p></li>
<li><p>投影映射到凸集中最接近原始点的点。</p></li>
</ul>
</div>
<div class="section" id="id16">
<h2><span class="section-number">11.2.5. </span>练习<a class="headerlink" href="#id16" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>假设我们想要通过绘制集合内点之间的所有直线并检查这些直线是否包含来验证集合的凸性。i.证明只检查边界上的点是充分的。ii.证明只检查集合的顶点是充分的。</p></li>
<li><p>用<span class="math notranslate nohighlight">\(p\)</span>-范数表示半径为<span class="math notranslate nohighlight">\(r\)</span>的球，证明<span class="math notranslate nohighlight">\(\mathcal{B}_p[r] := \{\mathbf{x} | \mathbf{x} \in \mathbb{R}^d \text{ and } \|\mathbf{x}\|_p \leq r\}\)</span>，<span class="math notranslate nohighlight">\(\mathcal{B}_p[r]\)</span>对于所有<span class="math notranslate nohighlight">\(p \geq 1\)</span>是凸的。</p></li>
<li><p>已知凸函数<span class="math notranslate nohighlight">\(f\)</span>和<span class="math notranslate nohighlight">\(g\)</span>表明<span class="math notranslate nohighlight">\(\mathrm{max}(f, g)\)</span>也是凸函数。证明<span class="math notranslate nohighlight">\(\mathrm{min}(f, g)\)</span>是非凸的。</p></li>
<li><p>证明Softmax函数的规范化是凸的，即<span class="math notranslate nohighlight">\(f(x) = \log \sum_i \exp(x_i)\)</span>的凸性。</p></li>
<li><p>证明线性子空间<span class="math notranslate nohighlight">\(\mathcal{X} = \{\mathbf{x} | \mathbf{W} \mathbf{x} = \mathbf{b}\}\)</span>是凸集。</p></li>
<li><p>证明在线性子空间<span class="math notranslate nohighlight">\(\mathbf{b} = \mathbf{0}\)</span>的情况下，对于矩阵<span class="math notranslate nohighlight">\(\mathbf{M}\)</span>的投影<span class="math notranslate nohighlight">\(\mathrm {Proj} \mathcal{X}\)</span>可以写成<span class="math notranslate nohighlight">\(\mathbf{M} \mathbf{X}\)</span>。</p></li>
<li><p>证明对于凸二次可微函数<span class="math notranslate nohighlight">\(f\)</span>，对于<span class="math notranslate nohighlight">\(\xi \in [0, \epsilon]\)</span>，我们可以写成<span class="math notranslate nohighlight">\(f(x + \epsilon) = f(x) + \epsilon f'(x) + \frac{1}{2} \epsilon^2 f''(x + \xi)\)</span>。</p></li>
<li><p>给定一个凸集<span class="math notranslate nohighlight">\(\mathcal{X}\)</span>和两个向量<span class="math notranslate nohighlight">\(\mathbf{x}\)</span>和<span class="math notranslate nohighlight">\(\mathbf{y}\)</span>证明了投影不会增加距离，即<span class="math notranslate nohighlight">\(\|\mathbf{x} - \mathbf{y}\| \geq \|\mathrm{Proj}_\mathcal{X}(\mathbf{x}) - \mathrm{Proj}_\mathcal{X}(\mathbf{y})\|\)</span>。</p></li>
</ol>
</div>
</div>


        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
<div class="localtoc">
    <p class="caption">
      <span class="caption-text">Table Of Contents</span>
    </p>
    <ul>
<li><a class="reference internal" href="#">11.2. 凸性</a><ul>
<li><a class="reference internal" href="#id3">11.2.1. 定义</a><ul>
<li><a class="reference internal" href="#id4">11.2.1.1. 凸集</a></li>
<li><a class="reference internal" href="#id5">11.2.1.2. 凸函数</a></li>
<li><a class="reference internal" href="#id6">11.2.1.3. 詹森不等式</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id7">11.2.2. 性质</a><ul>
<li><a class="reference internal" href="#id8">11.2.2.1. 局部极小值是全局极小值</a></li>
<li><a class="reference internal" href="#id9">11.2.2.2. 凸函数的下水平集是凸的</a></li>
<li><a class="reference internal" href="#id10">11.2.2.3. 凸性和二阶导数</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11">11.2.3. 约束</a><ul>
<li><a class="reference internal" href="#id12">11.2.3.1. 拉格朗日函数</a></li>
<li><a class="reference internal" href="#id13">11.2.3.2. 惩罚</a></li>
<li><a class="reference internal" href="#id14">11.2.3.3. 投影</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id15">11.2.4. 小结</a></li>
<li><a class="reference internal" href="#id16">11.2.5. 练习</a></li>
</ul>
</li>
</ul>

</div>
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
     <a id="button-prev" href="optimization-intro.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="P">
         <i class="pagenation-arrow-L fas fa-arrow-left fa-lg"></i>
         <div class="pagenation-text">
            <span class="pagenation-direction">Previous</span>
            <div>11.1. 优化和深度学习</div>
         </div>
     </a>
     <a id="button-next" href="gd.html" class="mdl-button mdl-js-button mdl-js-ripple-effect mdl-button--colored" role="botton" accesskey="N">
         <i class="pagenation-arrow-R fas fa-arrow-right fa-lg"></i>
        <div class="pagenation-text">
            <span class="pagenation-direction">Next</span>
            <div>11.3. 梯度下降</div>
        </div>
     </a>
  </div>
        
        </main>
    </div>
  </body>
</html>